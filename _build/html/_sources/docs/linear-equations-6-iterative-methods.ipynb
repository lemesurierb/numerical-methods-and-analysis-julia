{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "white-johns",
   "metadata": {},
   "source": [
    "# Iterative Methods for Simultaneous Linear Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-shadow",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "- Section 2.5 *Iterative Methods* in {cite}`Sauer`, sub-sections 2.5.1 to 2.5.3.\n",
    "- Chapter 7 *Iterative Techniques in Linear Algebra* in {cite}`Burden-Faires`, sections 7.1 to 7.3.\n",
    "- Section 8.4 in {cite}`Chenney-Kincaid`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-lightweight",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This topic is a huge area, with lots of ongoing research; this section just explores the first few methods in the field:\n",
    "\n",
    "1. The Jacobi Method.\n",
    "\n",
    "2. The Gauss-Seidel Method.\n",
    "\n",
    "The next three major topics for further study are:\n",
    "\n",
    "3. The Method of Succesive Over-Relaxation (\"SOR\").\n",
    "This is usually done as a modification of the Gauss-Seidel method, though the strategy of \"over-relaxation\" can also be applied to other iterative methods such as the Jacobi method.\n",
    "\n",
    "4. The Conjugate Gradient Method (\"CG\").\n",
    "This is beyond the scope of this course; I mention it because in the realm of solving linear systems that arise in the solution of differential equations, CG and SOR are the basis of many of the most modern, advanced methods.\n",
    "\n",
    "5. Preconditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-scroll",
   "metadata": {},
   "source": [
    "## The Jacobi method\n",
    "\n",
    "The basis of the Jacobi method for solving $Ax = b$\n",
    "is splitting $A$ as $D + R$ where $D$ is the diagonal of $A$:\n",
    "\n",
    "$$\\begin{split}\n",
    "d_{i,i} &= a_{i,i}\n",
    "\\\\\n",
    "d_{i,j} &= 0, \\quad i \\neq j\n",
    "\\end{split}$$\n",
    "\n",
    "so that $R = A-D$ has\n",
    "\n",
    "$$\\begin{split}\n",
    "r_{i,i} &= 0\n",
    "\\\\\n",
    "r_{i,j} &= a_{i, j}, \\quad i \\neq j\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-wheel",
   "metadata": {},
   "source": [
    "Visually\n",
    "\n",
    "$$\n",
    "D = \\left[ \\begin{array}{cccc}\n",
    "a_{11} & 0 & 0 & \\dots\n",
    "\\\\\n",
    "0 & a_{22} & 0 & \\dots\n",
    "\\\\\n",
    "0 & 0 & a_{33} & \\dots\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array} \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-translation",
   "metadata": {},
   "source": [
    "It is easy to solve $Dx = b$: the equations are just $a_{ii} x_i = b_i$ with solution $x_i = b_i/a_{ii}$.\n",
    "\n",
    "Thus we rewrite the equation\n",
    "$Ax = Dx + Rx = b$\n",
    "in the fixed point form\n",
    "\n",
    "$$Dx = b - Rx$$\n",
    "\n",
    "and then use the familiar fixed point iteration strategy of inserting the currect approximation at right and solving for the new approximation at left:\n",
    "\n",
    "$$D x^{(k)} = b - R x^{(k-1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-triple",
   "metadata": {},
   "source": [
    "**Note:** We could make this look closer to the standard fixed-point iteration form $x_k = g(x_{k-1})$ by dividing out $D$ to get\n",
    "\n",
    "$$x^{(k)} = D^{-1}(b - R x^{(k-1)}),$$\n",
    "\n",
    "but — as is often the case — it will be better to avoid matrix inverses by instead solving this easy system.\n",
    "This \"inverse avoidance\" becomes far more important when we get to the Gauss-Seidel method!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1323c1-cba2-4fd8-87cf-4db4a73abf08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "See [Exercise 1](#exercise-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-hundred",
   "metadata": {},
   "source": [
    "## The underlying strategy\n",
    "\n",
    "To analyse the Jacobi method — answering questions like for which matrices it works, and how quickly it converges — and also to improve on it, it helps to described a key strategy underlying it, which is this:\n",
    "approximate the matrix $A$ by another one $E$ one that is easier to solve with, chosen so that the discrepacy $R = A-E$ is small enough. Thus, repeatedly solving the new easier equations $Ex^{(k)} = b^{(k)}$ plays a similar role to repeatedly solving tangent line approximations in Newton's method.\n",
    "\n",
    "Of course to be of any use, $E$ must be somewhat close to $A$; the remainder $R$ must be small enough.\n",
    "We can make this requirement precise with the use of\n",
    "[matrix norms](linear-equations-5-error-bounds-condition-numbers.ipynb#matrix-norms)\n",
    "introduced in {doc}`linear-equations-5-error-bounds-condition-numbers`\n",
    "and an upgrade of the contraction mapping theorem seen in {doc}`fixed-point-iteration`.\n",
    "\n",
    "Thus consider a general *splitting* of $A$ as $A = E + R$.\n",
    "As above, we rewrite $Ax = Ex + Rx = b$ as $Ex = b - Rx$ and thence as $x = E^{-1}b - (E^{-1}R)x$.\n",
    "(It is alright to use the matrix inverse here, since we are not actually computing it; only using it for a theoretical argument!)\n",
    "The fixed point iteration form is thus\n",
    "\n",
    "$$x^{(k)} = g(x^{(k-1)}) = c - S x^{(k-1)}$$\n",
    "\n",
    "where $c = E^{-1}b$ and $S = E^{-1}R$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ca1217-cd48-41ef-82c5-59500aafcacd",
   "metadata": {},
   "source": [
    "For vector-valued functions we extend the previous {prf:ref}`definition-contraction-mapping`\n",
    "in Section {doc}`fixed-point-iteration` as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f9911-f5c8-4a20-b2c6-784b246aa966",
   "metadata": {},
   "source": [
    "```{prf:definition} Vector-valued contraction mapping\n",
    ":label: definition-vector-valued-contraction-mapping\n",
    "\n",
    "For a set $D$ of vectors in $\\mathbb{R}^n$, a mapping $g:D \\to D$ is called a *contraction* or *contraction mapping* if there is a constant $C < 1$ such that\n",
    "\n",
    "$$\\|g(x) - g(y)\\| \\leq C \\|x - y\\|$$\n",
    "\n",
    "for any $x$ and $y$ in $D$.\n",
    "We then call $C$ a *contraction constant*.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d17d4a-634e-4669-a60a-ae6cef93950e",
   "metadata": {},
   "source": [
    "Next, the contraction mapping theorem {prf:ref}`a-contraction-mapping-theorem` extends to\n",
    "\n",
    "```{prf:theorem} Contraction mapping theorem for vector-valued functions\n",
    ":label: vector-valued-contraction-mapping-theorem\n",
    "\n",
    "- Any contraction mapping $g$ on a closed, bounded set $D \\in \\mathbb{R}^n$ has exactly one fixed point $p$ in $D$.\n",
    "\n",
    "- This can be calculated as the limit $\\displaystyle p = \\lim_{k \\to \\infty} x^{(k)}$ of the iteration sequence given by $x^{(k)} = g(x^{(k-1)})$ for *any* choice of the starting point $x^{(0)} \\in D$.\n",
    "\n",
    "- The errors decrease at a guaranteed minimum speed:\n",
    "$\\| x^{(k)} - p \\| \\leq C \\| x^{(k-1)} - p \\|$, so $\\| x^{(k)} - p \\| \\leq C^k  \\| x^{(0)} - p \\|$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb1198-ddd0-4407-a6ca-80d6e0cee8e1",
   "metadata": {},
   "source": [
    "With this, it turns out that the above iteration converges if $S$ is \"small enough\" in the sense that $\\|S\\| = C < 1$ — and it is enough that this works for *any* choice of matrix norm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6f0d2-acab-4b04-8dc2-59e48ef88173",
   "metadata": {},
   "source": [
    "```{prf:theorem} \n",
    ":label: theorem-matrix-iteration-convergence\n",
    "\n",
    "If $S := E^{-1}R = E^{-1}A - I$ has $\\|S\\| = C < 1$ for any choice of matrix norm,\n",
    "then the iterative scheme $x^{(k)} = c - S x^{(k-1)}$ with $c = E^{-1}b$ converges to the solution of\n",
    "$Ax = b$ for any choice of the initial approximation $x^{(0)}$.\n",
    "(Aside: the zero vector is an obvious and popular choice for $x^{(0)}$.)\n",
    "\n",
    "Incidentally, since this condition guarantees that there exists a unique solution to $Ax=b$,\n",
    "it also shows that $A$ is non-singular. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355eacd8-3459-4214-b179-9f5e6faaf34a",
   "metadata": {},
   "source": [
    "```{prf:proof} (sketch)\n",
    "\n",
    "The main idea is that for $g(x) = c - S x$,\n",
    "\n",
    "$$\\| g(x) - g(y) \\| = \\| (c - S x) - (c - S y) \\| = \\| S(y - x) \\| \\leq \\| S \\| \\| y-x \\| \\leq C \\| x - y \\|,$$\n",
    "\n",
    "so with $C < 1$, it is a contraction.\n",
    "\n",
    "(The omitted more \"technical\" detail is to find a suitable bounded domain $D$ that all the iterates x^{(k)} stay inside it.)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-touch",
   "metadata": {},
   "source": [
    "### What does this say about the Jacobi method?\n",
    "\n",
    "For the Jacobi method, $E = D$ so $E^{-1}$ is the diagonal matrix with elements $1/a_{i,i}$ on the main diagonal, zero elsewhere.\n",
    "The product $E^{-1}A$ then multiplies each row $i$ of $A$ by $1/a_{i,i}$, giving\n",
    "\n",
    "$$\n",
    "E^{-1}A = \\left[ \\begin{array}{cccc}\n",
    "1 & a_{1,2}/a_{1,1} & a_{1,2}/a_{1,1} & \\dots\n",
    "\\\\\n",
    "a_{2,1}/a_{2,2} & 1 & a_{2,3}/a_{2,2} & \\dots\n",
    "\\\\\n",
    "a_{3,1}/a_{3,3} & a_{3,2}/a_{3,3} & 1 & \\dots\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "so that subtracting the identity matrix to get $S$ cancels the ones on the main diagonal:\n",
    "\n",
    "$$\n",
    "S = E^{-1}A - I= \\left[ \\begin{array}{cccc}\n",
    "0 & a_{1,2}/a_{1,1} & a_{1,2}/a_{1,1} & \\dots\n",
    "\\\\\n",
    "a_{2,1}/a_{2,2} & 0 & a_{2,3}/a_{2,2} & \\dots\n",
    "\\\\\n",
    "a_{3,1}/a_{3,3} & a_{3,2}/a_{3,3} & 0 & \\dots\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e470f8-05a9-4400-ab6a-fc00ec7b4ad9",
   "metadata": {},
   "source": [
    "Here is one of many places that using the maximum-norm, a.k.a. $\\infty$-norm, makes life much easier!\n",
    "Recalling that this is given by\n",
    "\n",
    "$$\n",
    "\\| A \\|_\\infty = \\max_{i=1}^n \\left( \\sum_{j=1}^n |a_{i,j}| \\right),\n",
    "$$\n",
    "\n",
    "- First, sum the absolute values of elements in each row $i$; with the common factor $1/|a_{i,i}|$,\n",
    "this gives\n",
    "$ \\left( |a_{i,1}| + |a_{i,2}| + \\cdots |a_{i,i-1}| + |a_{i,i+1}| + \\cdots |a_{i,n}| \\right)/|a_{i,i}| $.\n",
    "<br>\n",
    "Such a sum, skipping index $j=i$, can be abbreviated as\n",
    "\n",
    "$$\\left( \\sum_{1 \\leq j \\leq n, j \\neq i}|a_{i,j}| \\right) \\Big/ |a_{i,i}|$$\n",
    "\n",
    "- Then get the norm as the maximum of these:\n",
    "\n",
    "$$\n",
    "C = \\| E^{-1}A \\|_\\infty = \\max_{i=1}^n  \\left[ \\left( \\sum_{1 \\leq j \\leq n, j \\neq i}|a_{i,j}| \\right) \\Big/ |a_{i,i}| \\right]\n",
    "$$\n",
    "\n",
    "and the contraction condition $C < 1$ becomes the requirement that each of these $n$ \"row sums\" is less than 1:\n",
    "\n",
    "Multiplying each of the inequalities by the denominator $|a_{i,i}|$ gives $n$ conditions\n",
    "\n",
    "$$\\left( \\sum_{1 \\leq j \\leq n, j \\neq i}|a_{i,j}| \\right) < |a_{i,i}|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5bcfe2-aa83-4e45-99e4-9bd8846284b2",
   "metadata": {},
   "source": [
    "This is strict diagonal dominance, as in {prf:ref}`definition-strictly-diagonally-dominant`\n",
    "in the section {doc}`linear-equations-1-row-reduction`,\n",
    "and as discussed there, one way to think of this is that such a matrix $A$ is close to its main diagonal $D$,\n",
    "which is the intuitive condition that the approximation of $A$ by $D$ as done in the Jacobi method is \"good enough\".\n",
    "\n",
    "And indeed, combining this result with {prf:ref}`theorem-matrix-iteration-convergence` gives:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-lying",
   "metadata": {},
   "source": [
    "```{prf:theorem} Convergence of the Jacobi method\n",
    ":label: theorem-jacobi-convergence\n",
    "\n",
    "The Jacobi Method converges if $A$ is strictly diagonally dominant, for any initial approximation $x^{(0)}$.\n",
    "\n",
    "Further, the error goes down by at least a factor of $\\| I - D^{-1}A \\|$ at each iteration.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-intranet",
   "metadata": {},
   "source": [
    "By the way, other matrix norms give other conditions guaranteeing convergence; perhaps the most useful of these others is that it is also sufficient for $A$ to be column-wise strictly diagonally dominant as in\n",
    "{prf:ref}`definition-columnwise-strictly-diagonally-dominant`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e2c17f-80ff-44a8-a57f-f0fcd65a0c05",
   "metadata": {},
   "source": [
    "## The Gauss-Seidel method\n",
    "\n",
    "To recap, two key ingredients for a splitting $A = E + R$ to be useful are that\n",
    "- the matrix $E$ is \"easy\" to solve with, and\n",
    "- it is not too far from $A$.\n",
    "\n",
    "The Jacobi method choice of $E$ being the main diagonal of $A$ strongly emphasizes the \"easy\" part, but we have seen another larger class of matrices for which it is fairly quick and easy to solve $Ex = b$: *triangular matrices*, which can be solved with forward or backward substitution, not needing row reduction.\n",
    "\n",
    "The Gauss-Seidel Method takes $E$ be the lower triangular part of $A$, which intuitively leaves more of its entries closer to $A$ and makes the remainder $R = A-E$ \"smaller\".\n",
    "\n",
    "To discuss this and other splittings, we write the matrix as $A = L + D + U$ where:\n",
    "- $D$ is the diagonal of $A$, as for Jacobi\n",
    "- $L$ is the strictly lower diagonal part of $A$ (just the elements with $i > j$)\n",
    "- $U$ is the strictly upper diagonal part of $A$ (just the elements with $i < j$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b081ae-5ba8-437c-85d5-9d2f9297af7a",
   "metadata": {},
   "source": [
    "That is,\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\left[ \\begin{array}{cccc}\n",
    "a_{1,1} & a_{1,2} & a_{1,3} & \\dots\n",
    "\\\\\n",
    "a_{2,1} & a_{2,2} & a_{2,3} & \\dots\n",
    "\\\\\n",
    "a_{3,1} & a_{3,2} & a_{3,3} & \\dots\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array} \\right]\n",
    "= \\left[ \\begin{array}{cccc}\n",
    "0 & 0 & 0 & \\dots\n",
    "\\\\\n",
    "a_{2,1} & 0 & 0 & \\dots\n",
    "\\\\\n",
    "a_{3,1} & a_{3,2} & 0 & \\dots\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array} \\right]\n",
    "+ \\left[ \\begin{array}{cccc}\n",
    "a_{1,1} & 0 & 0 & \\dots\n",
    "\\\\\n",
    "0 & a_{2,2} & 0 & \\dots\n",
    "\\\\\n",
    "0 & 0 & a_{3,3} & \\dots\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array} \\right]\n",
    "+\n",
    "\\left[ \\begin{array}{cccc}\n",
    "0 & a_{1,2} & a_{1,3} & \\dots\n",
    "\\\\\n",
    "0 & 0 & a_{2,3} & \\dots\n",
    "\\\\\n",
    "0 & 0 & 0 & \\dots\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array} \\right]\n",
    "= L + D + U\n",
    "$$\n",
    "\n",
    "Thus $R = L+U$ for the Jacobi method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-latter",
   "metadata": {},
   "source": [
    "So now we use $E = L + D$, which will be called $A_L$, the *lower triangular part of $A$*, and the remainder is $R = U$.\n",
    "The fixed point form becomes\n",
    "\n",
    "$$A_L x = b - Ux$$\n",
    "\n",
    "giving the fixed point iteration\n",
    "\n",
    "$$A_L x^{(k)} = b - U x^{(k-1)}$$\n",
    "\n",
    "Here we definitely do not use the inverse of $A_L$ when calculating!\n",
    "Instead, solve with forward substitution.\n",
    "\n",
    "However to analyse convergence, the mathematical form\n",
    "\n",
    "$$x^{(k)} = A_L^{-1}b - (A_L^{-1}U) x^{(k-1)}$$\n",
    "\n",
    "is useful: the iteration map is now $g(x) = c - S x$ with $c = (L + D)^{-1} b$ and $S = (L + D)^{-1} U$.\n",
    "\n",
    "Arguing as above, we see that convergence is guaranteed if $\\| (L + D)^{-1} U\\| < 1$.\n",
    "However it is not so easy in general to get a formula for $\\| (L + D)^{-1} U\\|$;\n",
    "what one can get is slightly disappointing in that, despite the $R=U$ here being in some sense \"smaller\" than the $R=L+U$ for the Jacobi method, the general convergence guarantee looks no better:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-legislation",
   "metadata": {},
   "source": [
    "```{prf:theorem} Convergence of the Gasuss-Seidel method\n",
    ":label: theorem-gaus-seidel-convergence\n",
    "\n",
    "The Gauss-Seidel method converges if $A$ is strictly diagonally dominant, for any initial approximation $x^{(0)}$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-puppy",
   "metadata": {},
   "source": [
    "However, in practice the convergence rate as given by $C = C_{GS} = \\| (L + D)^{-1} U\\|$ is often better than for the\n",
    "$C = C_J = \\| D^{-1} (L+U) \\|$ for the Jacobi method.\n",
    "\n",
    "Sometimes this reduces the number of iterations enough to outweigh the extra computational effort involved in each iteration and make this faster overall than the Jacobi method — but not always."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fef11e-d2fa-4e49-8511-75128c09e0f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "See [Exercise 2](#exercise-2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-alpha",
   "metadata": {},
   "source": [
    "## A family of test cases, arising from boundary value problems for differential equations\n",
    "\n",
    "The **tri-diagonal** matrices $T$ of the form\n",
    "\n",
    "$$\\begin{split}\n",
    "t_{i,i} &= 1 + 2 h^2\n",
    "\\\\\n",
    "t_{i,i+1} = t_{i,i+1} &= -h^2\n",
    "\\\\\n",
    "t_{i,j} &= 0, \\quad |i-j|> 1\n",
    "\\end{split}$$\n",
    "\n",
    "and variants of this arise in the solutions of boundary value problems for ODEs like\n",
    "\n",
    "$$\\begin{split}\n",
    "-u''(x) + K u &= f(x), \\quad a \\leq x \\leq b\n",
    "\\\\\n",
    "u(a) = u(b) &= 0\n",
    "\\end{split}$$\n",
    "\n",
    "and related problems for partial differential equations.\n",
    "\n",
    "Thus these provide useful initial test cases — usually with $h = (b-a)/n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f8414-fdf1-4502-9702-b064989d8736",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b07c70-f884-4f57-a0e5-3283b8026454",
   "metadata": {},
   "source": [
    "<a name=\"exercise-1\"></a>\n",
    "### Exercise 1: Implement and test the Jacobi method\n",
    "\n",
    "Write and test Julia functions for this.\n",
    "\n",
    "A) As usual start with a most basic version that does a fixed number of iterations\n",
    "\n",
    "    x = jacobiBasic(A, b, n)\n",
    "    \n",
    "B) Then refine this to apply an error tolerance, but also avoiding infinite loops by imposing an upper limit on the number of iterations:\n",
    "\n",
    "    x = jacobi(A, b, errorTolerance, maxIterations)\n",
    "    \n",
    "Test this with the matrices of form $T$ below for several values of $n$, increasingly geometrically.\n",
    "To be cautious initially, try $n=2, 4, 8, 16, \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd3262-a4b2-48d0-b16f-9b991ae67b1a",
   "metadata": {},
   "source": [
    "<a name=\"exercise-2\"></a>\n",
    "### Exercise 2: Implement and test the Gauss-Seidel method, and compare to Jacobi\n",
    "\n",
    "Do the two versions as above and use the same test cases.\n",
    "\n",
    "Then compare the speed/cost of the two methods:\n",
    "one way to do this is by using Julia's \"wall clock time\" function\n",
    "[`time`](https://docs.julialang.org/en/v1/base/base/#Base.Libc.time-Tuple{})\n",
    "or the macro\n",
    "[`@time`](https://docs.julialang.org/en/v1/manual/profile/#@time);\n",
    "see the linked descriptions in the Julia manual."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
