
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>3.9. Computing Eigenvalues and Eigenvectors: the Power Method, and a bit beyond &#8212; Introduction to Numerical Methods and Analysis with Julia (draft)</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3.10. Solving Nonlinear Systems of Equations by generalizations of Newton’s Method — a brief introduction" href="newtons-method-for-systems-intro.html" />
    <link rel="prev" title="3.8. Faster Methods for Solving \(Ax = b\) for Tridiagonal and Banded matrices, and Strict Diagonal Dominance" href="linear-equations-7-tridiagonal-banded-and-SDD-matrices.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/WM_SSM_new_logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Numerical Methods and Analysis with Julia (draft)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Preface
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="root-finding.html">
   2. Root-finding
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="root-finding-by-interval-halving.html">
     2.1. Root Finding by Interval Halving (Bisection)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fixed-point-iteration.html">
     2.2. Solving Equations by Fixed Point Iteration (of Contraction Mappings)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="newtons-method.html">
     2.3. Newton’s Method for Solving Equations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="taylors-theorem.html">
     2.4. Taylor’s Theorem and the Accuracy of Linearization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="error-measures-convergence-rates.html">
     2.5. Measures of Error and Order of Convergence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="newtons-method-convergence-rate.html">
     2.6. The Convergence Rate of Newton’s Method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="root-finding-without-derivatives.html">
     2.7. Root-finding without Derivatives
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="linear-algebra.html">
   3. Linear Algebra and Simultaneous Equations
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="linear-equations-1-row-reduction.html">
     3.1. Row Reduction/Gaussian Elimination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="machine-numbers-rounding-error-and-error-propagation.html">
     3.2. Machine Numbers, Rounding Error and Error Propagation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linear-equations-2-pivoting.html">
     3.3. Partial Pivoting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linear-equations-3-lu-factorization.html">
     3.4. Solving
     <span class="math notranslate nohighlight">
      \(Ax = b\)
     </span>
     with LU factorization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linear-equations-4-plu-factorization.html">
     3.5. Solving
     <span class="math notranslate nohighlight">
      \(Ax = b\)
     </span>
     With Both Pivoting and LU Factorization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linear-equations-5-error-bounds-condition-numbers.html">
     3.6. Error bounds for linear algebra, condition numbers, matrix norms, etc.
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linear-equations-6-iterative-methods.html">
     3.7. Iterative Methods for Simultaneous Linear Equations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linear-equations-7-tridiagonal-banded-and-SDD-matrices.html">
     3.8. Faster Methods for Solving
     <span class="math notranslate nohighlight">
      \(Ax = b\)
     </span>
     for Tridiagonal and Banded matrices, and Strict Diagonal Dominance
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     3.9. Computing Eigenvalues and Eigenvectors: the Power Method, and a bit beyond
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="newtons-method-for-systems-intro.html">
     3.10. Solving Nonlinear Systems of Equations by generalizations of Newton’s Method — a brief introduction
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="collocation-approximation.html">
   4. Polynomial Collocation and Approximation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="polynomial-collocation%2Bapproximation.html">
     4.1. Polynomial Collocation (Interpolation/Extrapolation) and Approximation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="polynomial-collocation-error-formulas.html">
     4.2. Error Formulas for Polynomial Collocation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="polynomial-collocation-chebychev.html">
     4.3. Choosing the collocation points: the Chebyshev method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="piecewise-polynomial-approximation-and-splines.html">
     4.4. Piecewise Polynomial Approximating Functions: Splines and Hermite Cubics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="least-squares-fitting.html">
     4.5. Least-Squares Fitting to Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="least-squares-fitting-appendix-geometrical-approach.html">
     4.6. Least-squares Fitting to Data: Appendix on The Geometrical Approach
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="calculus.html">
   5. Derivatives and Definite Integrals
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="derivatives-and-the-method-of-undetermined-coefficents.html">
     5.1. Approximating Derivatives by the Method of Undetermined Coefficients
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="richardson-extrapolation.html">
     5.2. Richardson Extrapolation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="integrals-1-building-blocks.html">
     5.3. Definite Integrals, Part 1: The Building Blocks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="integrals-2-composite-rules.html">
     5.4. Definite Integrals, Part 2: The Composite Trapezoid and Midpoint Rules
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="integrals-3-simpson-richardson.html">
     5.5. Definite Integrals, Part 3: The (Composite) Simpson’s Rule and Richardson Extrapolation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="integrals-4-romberg-integration.html">
     5.6. Definite Integrals, Part 4: Romberg Integration
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="minimization.html">
   6. Minimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="minimization-1D.html">
     6.1. Finding the Minimum of a Function of One Variable Without Using Derivatives – under construction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="minimization-multidimensional-stub.html">
     6.2. Finding the Minimum of a Function of Several Variables — Coming Soon
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="ODE-IVPs.html">
   7. Initial Value Problems for Ordinary Differential Equations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="ODE-IVP-0-background-examples.html">
     7.1. Background and Some Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ODE-IVP-1-Euler.html">
     7.2. Euler’s Method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ODE-IVP-2-Runge-Kutta.html">
     7.3. Runge-Kutta Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ODE-IVP-3-error-results-one-step-methods.html">
     7.4. A Global Error Bound for One Step Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ODE-IVP-4-system-higher-order-equations.html">
     7.5. Systems of ODEs and Higher Order ODEs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ODE-IVP-5-error-control.html">
     7.6. Error Control and Variable Step Sizes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ODE-IVP-6-multi-step-methods-introduction.html">
     7.7. An Introduction to Multistep Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ODE-IVP-7-multi-step-methods-Adams-Bashforth.html">
     7.8. Adams-Bashforth Multistep Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ODE-IVP-8-implicit-methods-Adams-Moulton.html">
     7.9. Implicit Methods: Adams-Moulton
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   8. Bibliography
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="appendices.html">
   9. Appendices
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="installing-julia-and-packages.html">
     9.1. Installing Julia and some useful add-ons
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="julia-language-notes.html">
     9.2. Notes on the Julia Language
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="NumericalMethods.html">
     9.3. Module
     <code class="docutils literal notranslate">
      <span class="pre">
       NumericalMethods
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/eigenproblems.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/docs/eigenproblems.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-power-method">
   The Power Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#refinement-deciding-the-iteration-count">
     Refinement: deciding the iteration count
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-inverse-power-method">
   The Inverse Power Method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#getting-other-eigenvalues-with-the-shifted-inverse-power-method">
   Getting other eigenvalues with the shifted inverse power method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-topics-getting-all-the-eigenvalues-with-the-qr-method-etc">
   Further topics: getting all the eigenvalues with the QR Method, etc.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-1">
     Exercise 1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-2">
     Exercise 2
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-3">
     Exercise 3
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-4">
     Exercise 4
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Computing Eigenvalues and Eigenvectors: the Power Method, and a bit beyond</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-power-method">
   The Power Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#refinement-deciding-the-iteration-count">
     Refinement: deciding the iteration count
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-inverse-power-method">
   The Inverse Power Method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#getting-other-eigenvalues-with-the-shifted-inverse-power-method">
   Getting other eigenvalues with the shifted inverse power method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-topics-getting-all-the-eigenvalues-with-the-qr-method-etc">
   Further topics: getting all the eigenvalues with the QR Method, etc.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-1">
     Exercise 1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-2">
     Exercise 2
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-3">
     Exercise 3
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-4">
     Exercise 4
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="computing-eigenvalues-and-eigenvectors-the-power-method-and-a-bit-beyond">
<h1><span class="section-number">3.9. </span>Computing Eigenvalues and Eigenvectors: the Power Method, and a bit beyond<a class="headerlink" href="#computing-eigenvalues-and-eigenvectors-the-power-method-and-a-bit-beyond" title="Permalink to this headline">#</a></h1>
<p><strong>References:</strong></p>
<ul class="simple">
<li><p>Section 12.1 <em>Power Iteration Methods</em> of <span id="id1">[<a class="reference internal" href="bibliography.html#id4" title="Timothy Sauer. Numerical Analysis. Pearson, 3rd edition, 2019.">Sauer, 2019</a>]</span>.</p></li>
<li><p>Section 7.2 <em>Eigenvalues and Eigenvectors</em> of <span id="id2">[<a class="reference internal" href="bibliography.html#id5" title="Richard L. Burden, J. Douglas Faires, and Annette M. Burden. Numerical Analysis. Cengage, 10th edition, 2016.">Burden <em>et al.</em>, 2016</a>]</span>.</p></li>
<li><p>Chapter 8, <em>More on Linear Equations</em> of <span id="id3">[<a class="reference internal" href="bibliography.html#id2" title="Ward Chenney and David Kincaid. Numerical Mathematics and Computing. Cengage, 7 edition, 2012.">Chenney and Kincaid, 2012</a>]</span>,
in particular section 3 <em>Power Method</em>, and also section 2 <em>Eigenvalues and Eigenvectors</em> as background reading.</p></li>
</ul>
<p>The <em>eigenproblem</em> for a square <span class="math notranslate nohighlight">\(n \times n\)</span> matrix <span class="math notranslate nohighlight">\(A\)</span> is to compute some or all non-trivial solutions of</p>
<div class="math notranslate nohighlight">
\[A \vec{v} = \lambda \vec{v}.\]</div>
<p>(By non-trivial, I mean to exclude <span class="math notranslate nohighlight">\(\vec{v} = 0\)</span>, which gives a solution for any <span class="math notranslate nohighlight">\(\lambda\)</span>.)
That is, to compute
the eigenvalues <span class="math notranslate nohighlight">\(\lambda\)</span> (of which generically there are <span class="math notranslate nohighlight">\(n\)</span>, but sometimes less)
and the eigenvectors <span class="math notranslate nohighlight">\(\vec{v}\)</span> corresponding to each.</p>
<p>With eigenproblems, and particularly those arising from differential equations, one often needs only the few smallest and/or largest eigenvalues. For these, the <em>power method</em> described next can be adapted, leading to the <em>shifted inverse power method</em>.</p>
<p>Here we often restict our attention to the case of a real <em>symmetric</em> matrix (<span class="math notranslate nohighlight">\(A^T = A\)</span>, or <span class="math notranslate nohighlight">\(A_{ij} = A_{ji}\)</span>), or a Hermitian matrix (<span class="math notranslate nohighlight">\(A^T = A^*\)</span>), for which many things are a bit simpler:</p>
<ul class="simple">
<li><p>all eigenvalues are real,</p></li>
<li><p>for symmetric matrices, all eigenvectors are also real,</p></li>
<li><p>there is a complete set of orthogonal eigenvectors <span class="math notranslate nohighlight">\(\vec{v}_k\)</span>, <span class="math notranslate nohighlight">\(1 \leq i \leq n\)</span> that form a basis for all vectors,
and so on.</p></li>
</ul>
<p>However, the methods described here can be used more generally, or can be made to work with minor adjustments.</p>
<p>The eigenvalue are roots of the characteristic polynomial, <span class="math notranslate nohighlight">\(\det(A - \lambda I)\)</span>;
repeated roots are possible, and they will all be named, so there are always values <span class="math notranslate nohighlight">\(\lambda_i\)</span>, <span class="math notranslate nohighlight">\(1 \leq i \leq n\)</span>.
Here, these eigenvalues will be enumerated in decreasing order of magnitude:</p>
<div class="math notranslate nohighlight">
\[|\lambda_1| \geq |\lambda_2| \cdots \geq |\lambda_n|.\]</div>
<p>Generically, all the magnitudes are different, which makes things works more easily,
so that will sometimes be assumed while developing the intuition of the method.</p>
<section id="the-power-method">
<h2>The Power Method<a class="headerlink" href="#the-power-method" title="Permalink to this headline">#</a></h2>
<p>The basic tool is the <em>Power Method</em>, which will usually <em>but not always</em> succeed in computing the eigenvalue of largest magnitude, <span class="math notranslate nohighlight">\(\lambda_1\)</span>, and <strong>a</strong> corresponding eigenvector <span class="math notranslate nohighlight">\(\vec{v}_1\)</span>.
Its success mainly involves assuming there being a unique largest eigenvalue: <span class="math notranslate nohighlight">\(\lambda_1 &gt; \lambda_i\)</span> for <span class="math notranslate nohighlight">\(i&gt;1\)</span>.</p>
<p>In its simplest form, one starts with a unit-length vector <span class="math notranslate nohighlight">\(\vec{x}^{\,0}\)</span>,
so <span class="math notranslate nohighlight">\(\|\vec{x}^{\,0}\| = 1\)</span>, constructs the successive multiples
<span class="math notranslate nohighlight">\(\vec{y}^{\,k} = A^k \vec{x}^{\,0}\)</span>
by successive multiplications, and rescales at each stage to the unit vectors
<span class="math notranslate nohighlight">\(\vec{x}^{\,k} = \vec{y}^{\,k}/\|\vec{y}^{\,k}\|\)</span>.</p>
<p>Note that <span class="math notranslate nohighlight">\(\vec{y}^{\,k+1} = A \vec{x}^{\,k}\)</span>, so that once <span class="math notranslate nohighlight">\(\vec{x}^{\,k}\)</span> is approximately an eigenvector for eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>, <span class="math notranslate nohighlight">\(\vec{y}^{\,k+1} \approx \lambda \vec{x}^{\,k}\)</span>, leading to the eigenvalue approximation</p>
<div class="math notranslate nohighlight">
\[r^{(k)} := \langle\vec{y}^{\,k+1}, \vec{x}^{\,k} \rangle
\approx \langle\lambda \vec{x}^{\,k}, \vec{x}^{\,k}\rangle \approx \lambda\]</div>
<div class="proof remark admonition" id="julia-dot">
<p class="admonition-title"><span class="caption-number">Remark 3.20 </span> (dot products in Julia)</p>
<section class="remark-content" id="proof-content">
<p>Here and below, I use <span class="math notranslate nohighlight">\(\langle \vec{a}, \vec{b} \rangle\)</span> to denote the inner product
(a.k.a. <em>dot product</em> or <em>scalar product</em>) of two vectors.</p>
<p>With Julia arrays this is given by function
<a class="reference external" href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.dot">dot(a,b)</a>
from package
<a class="reference external" href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/">LinearAlgebra</a>,
obtained with</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>using LinearAlgebra: dot
</pre></div>
</div>
<p>You can even type this as</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>a ⋅ b
</pre></div>
</div>
<p>(To get that centered dot in Julia, type \cdot and then tab.)</p>
</section>
</div><div class="proof algorithm admonition" id="power-method">
<p class="admonition-title"><span class="caption-number">Algorithm 3.15 </span> (A basic version of the power method)</p>
<section class="algorithm-content" id="proof-content">
<p><em>Choose</em> initial vector <span class="math notranslate nohighlight">\(\vec{y}_0\)</span>, maybe with a random number generator.</p>
<p>Normalize to <span class="math notranslate nohighlight">\(\vec{x}^{\,0} = \vec{y}^{\,0}/\|\vec{y}^{\,0}\|\)</span>.</p>
<p>for <span class="math notranslate nohighlight">\(k\)</span> from 0 to <span class="math notranslate nohighlight">\(k_{max}\)</span>
<br>
<span class="math notranslate nohighlight">\(\quad\)</span> <span class="math notranslate nohighlight">\(\vec{y}^{\,k+1} = A \vec{x}^{\,k}\)</span>
<br>
<span class="math notranslate nohighlight">\(\quad\)</span> <span class="math notranslate nohighlight">\(r^{(k)} = \langle \vec{y}^{\,k+1}, \vec{x}^{\,k} \rangle\)</span>
<br>
<span class="math notranslate nohighlight">\(\quad\)</span> <span class="math notranslate nohighlight">\(\vec{x}^{\,k+1} = \vec{y}^{\,k+1}/\|\vec{y}^{\,k+1}\|\)</span>
<br>
end</p>
<p>The final values of <span class="math notranslate nohighlight">\(r^{(k)}\)</span> and <span class="math notranslate nohighlight">\(\vec{x}^{\,k}\)</span> approximate <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\vec{v}_1\)</span> respectively.</p>
</section>
</div><p>See <a class="reference internal" href="#exercise-1"><span class="std std-ref">Exercise 1</span></a>.</p>
<p>Here and below you could check your work with Julia, using function
<a class="reference external" href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigvals">eigvals</a>
from package <a class="reference external" href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/">LinearAlgebra</a>.</p>
<p>However, that is almost cheating, so note that there is also a backward error check:
see how small <span class="math notranslate nohighlight">\(\| A v - \lambda v \| / \| v \|\)</span> is.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span> <span class="n">LinearAlgebra</span><span class="o">:</span> <span class="n">eigvals</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">include</span><span class="p">(</span><span class="s">&quot;NumericalMethods.jl&quot;</span><span class="p">)</span>
<span class="k">using</span> <span class="o">.</span><span class="n">NumericalMethods</span><span class="o">:</span> <span class="n">printmatrix</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">delta</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">A</span> <span class="o">=</span> <span class="p">[</span><span class="mf">3.0</span> <span class="n">delta</span> <span class="n">delta</span> <span class="p">;</span> <span class="n">delta</span> <span class="mf">8.0</span> <span class="n">delta</span> <span class="p">;</span> <span class="n">delta</span> <span class="n">delta</span> <span class="mf">4.0</span><span class="p">]</span>
<span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">eigvals</span><span class="p">(</span><span class="n">A</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">println</span><span class="p">(</span><span class="s">&quot;With delta=</span><span class="si">$</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span><span class="s">, so that A is&quot;</span><span class="p">);</span> <span class="n">printmatrix</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;the eigenvalues are </span><span class="si">$</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span><span class="s">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>With delta=0.01, so that A is
[ 3.0 0.01 0.01 
  0.01 8.0 0.01 
  0.01 0.01 4.0 ]
the eigenvalues are [2.999880409987068, 4.000074490251736, 8.00004509976119]
</pre></div>
</div>
</div>
</div>
<div class="proof remark admonition" id="julia-eigenvec">
<p class="admonition-title"><span class="caption-number">Remark 3.21 </span> (On Julia)</p>
<section class="remark-content" id="proof-content">
<p>That package LinearAlgebra also has a function
<a class="reference external" href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigvecs">eigvecs</a>
for computing eignevevctors.</p>
<p>It can compute them “from scratch” with <code class="docutils literal notranslate"><span class="pre">eigenvectors</span> <span class="pre">=</span> <span class="pre">eigvecs(A)</span></code>,
which returns them in the columns of that matrix <code class="docutils literal notranslate"><span class="pre">eigenvectors</span></code>.</p>
<p>However, if you already have the eigenvalues, the calculation is much quicker by using them:
that is done with
<code class="docutils literal notranslate"><span class="pre">eigenvectors</span> <span class="pre">=</span> <span class="pre">eigvecs(A,</span> <span class="pre">eigenvalues)</span></code></p>
</section>
</div><section id="refinement-deciding-the-iteration-count">
<h3>Refinement: deciding the iteration count<a class="headerlink" href="#refinement-deciding-the-iteration-count" title="Permalink to this headline">#</a></h3>
<p>Some details are omitted above; above all, how to decide the number of iterations.</p>
<p>One approach is to use the fact that an eigenvector-eigenvalue pair satisfies
<span class="math notranslate nohighlight">\(A \vec{v} - \lambda \vec{v} = 0\)</span>, so the “residual norm”</p>
<div class="math notranslate nohighlight">
\[
\frac{\|A \vec{x}^{\,k} - r^{(k)} \vec{x}^{\,k}\|}{\|\vec{x}^{\,k}\|},
= \|\vec{y}^{\,k+1} - r^{(k)} \vec{x}^{\,k}\| \text{ since $\|\vec{x}^{\,k}\| = 1$}
\]</div>
<p>is a measure of “relative backward error”.</p>
<p>Thus one could repace the above <code class="docutils literal notranslate"><span class="pre">for</span></code> loop by a <code class="docutils literal notranslate"><span class="pre">while</span></code> loop based on a condition like stopping when</p>
<div class="math notranslate nohighlight" id="equation-a-stopping-condition">
<span class="eqno">(3.4)<a class="headerlink" href="#equation-a-stopping-condition" title="Permalink to this equation">#</a></span>\[ \|\vec{y}^{\,k+1} - r^{(k)} \vec{x}^{\,k}\| \leq \epsilon. \]</div>
<p>Alternatively, keep the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop, but exit early (with <code class="docutils literal notranslate"><span class="pre">break</span></code>) if this condition is met.</p>
<p>I generally recommend this <code class="docutils literal notranslate"><span class="pre">for-if-break</span></code> form for implementing iterative methods, because it makes avoidance of infinite loops simpler, and avoids the common <code class="docutils literal notranslate"><span class="pre">while</span></code> loop issue that you do not yet have an error estimate when the loop starts.</p>
<p>See <a class="reference internal" href="#exercise-2"><span class="std std-ref">Exercise 2</span></a>.</p>
</section>
</section>
<section id="the-inverse-power-method">
<h2>The Inverse Power Method<a class="headerlink" href="#the-inverse-power-method" title="Permalink to this headline">#</a></h2>
<p>The next step is to note that if <span class="math notranslate nohighlight">\(A\)</span> is nonsingular, its inverse <span class="math notranslate nohighlight">\(B = A^{-1}\)</span> has the same eigenvectors, but with eigenvalues <span class="math notranslate nohighlight">\(\mu_i = 1/\lambda_i\)</span>.</p>
<p>Thus we can apply the power method to <span class="math notranslate nohighlight">\(B\)</span> in order to compute its largest eigenvalue, which is <span class="math notranslate nohighlight">\(\mu_n = 1/\lambda_n\)</span>, along with the corresponding eigenvector <span class="math notranslate nohighlight">\(\vec{v}_n\)</span>.</p>
<p>The main change to the above is that</p>
<div class="math notranslate nohighlight">
\[\vec{y}^{\,k+1} = A^{-1} \vec{x}_{k}.\]</div>
<p>However, as usual one can (and should) avoid actually computing the inverse.
Instead, express the above as the sysem of equations.</p>
<div class="math notranslate nohighlight">
\[A \vec{y}^{\,k+1} = \vec{x}_{k}.\]</div>
<p>Here is an important case where the LU factorization method can speed things up greatly: a single LU factorization is needed, after which for each <span class="math notranslate nohighlight">\(k\)</span> one only has to do the far quicker forward and backward substitution steps: <span class="math notranslate nohighlight">\(O(n^2)\)</span> cost for each iteration instead of <span class="math notranslate nohighlight">\(O(n^3/3)\)</span>.</p>
<div class="proof algorithm admonition" id="inverse-power-method">
<p class="admonition-title"><span class="caption-number">Algorithm 3.16 </span> (A basic version of the inverse power method)</p>
<section class="algorithm-content" id="proof-content">
<p><em>Choose</em> initial vector <span class="math notranslate nohighlight">\(\vec{y}_0\)</span>, maybe with a random number generator.</p>
<p>Normalize to <span class="math notranslate nohighlight">\(\vec{x}^{\,0} = \vec{y}^{\,0}/\|\vec{y}^{\,0}\|\)</span>.</p>
<p>Compute an LU factorization <span class="math notranslate nohighlight">\(L U = A\)</span>.</p>
<p>for <span class="math notranslate nohighlight">\(k\)</span> from 0 to <span class="math notranslate nohighlight">\(k_{max}\)</span>
<br>
<span class="math notranslate nohighlight">\(\quad\)</span> Solve <span class="math notranslate nohighlight">\(L \vec{z}^{\,k+1} = \vec{x}^{\,k}\)</span>
<br>
<span class="math notranslate nohighlight">\(\quad\)</span> Solve <span class="math notranslate nohighlight">\(U \vec{y}^{\,k+1} = \vec{z}^{\,k+1}\)</span>
<br>
<span class="math notranslate nohighlight">\(\quad\)</span> <span class="math notranslate nohighlight">\(r^{(k)} = \langle \vec{y}^{\,k+1}, \vec{x}^{\,k} \rangle\)</span>
<br>
<span class="math notranslate nohighlight">\(\quad\)</span> <span class="math notranslate nohighlight">\(\vec{x}^{\,k+1} = \vec{y}^{\,k+1}/\| \vec{y}^{\,k+1} \|\)</span>
<br>
end</p>
<p>(If all goes well) the final values of <span class="math notranslate nohighlight">\(r^{(k)}\)</span> and <span class="math notranslate nohighlight">\(\vec{x}^{\,k}\)</span> approximate <span class="math notranslate nohighlight">\(\lambda_n\)</span> and <span class="math notranslate nohighlight">\(\vec{v}_n\)</span> respectively.</p>
</section>
</div><p>See <a class="reference internal" href="#exercise-3"><span class="std std-ref">Exercise 3</span></a>.</p>
</section>
<section id="getting-other-eigenvalues-with-the-shifted-inverse-power-method">
<span id="shifted-inverse-power-method"></span><h2>Getting other eigenvalues with the shifted inverse power method<a class="headerlink" href="#getting-other-eigenvalues-with-the-shifted-inverse-power-method" title="Permalink to this headline">#</a></h2>
<p>The inverse power method computes the eigenvalue closest to 0; by <em>shifting</em>, we can compute the eigenvalue closest to any chosen value <span class="math notranslate nohighlight">\(s\)</span>.
Then by searching various values of <span class="math notranslate nohighlight">\(s\)</span>, we can hope to find all the eigenvectors.
As a variant, once we have <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\lambda_n\)</span>, we can search nearby for other large or small eigenvalues: often the few largest and/or the few smallest are most important.</p>
<p>With a symmetric (or Hermitian) matrix, once the eigenvalue of largest magnitude, <span class="math notranslate nohighlight">\(\lambda_1\)</span> is known, the rest are known to be real values in the interval <span class="math notranslate nohighlight">\([-|\lambda_1|,|\lambda_1|]\)</span>, so we know roughly where to seek them.</p>
<p>The main idea here is that for any number <span class="math notranslate nohighlight">\(s\)</span>, matrix <span class="math notranslate nohighlight">\(A - s I\)</span> has eigenvalues
<span class="math notranslate nohighlight">\(\lambda_i  - s\)</span>, with the same eigenvectors as <span class="math notranslate nohighlight">\(A\)</span>:</p>
<div class="math notranslate nohighlight">
\[(A - sI)\vec{v}_i = (\lambda_i  - s)\vec{v}_i\]</div>
<p>Thus, applying the inverse power method to <span class="math notranslate nohighlight">\(A - s I\)</span> computes its largest eigenvalue <span class="math notranslate nohighlight">\(\gamma\)</span>,
and then <span class="math notranslate nohighlight">\(\lambda = 1/(\gamma + s)\)</span> is the eigenvalue of <span class="math notranslate nohighlight">\(A\)</span> closest to <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>See <a class="reference internal" href="#exercise-4"><span class="std std-ref">Exercise 4</span></a>.</p>
</section>
<section id="further-topics-getting-all-the-eigenvalues-with-the-qr-method-etc">
<h2>Further topics: getting all the eigenvalues with the QR Method, etc.<a class="headerlink" href="#further-topics-getting-all-the-eigenvalues-with-the-qr-method-etc" title="Permalink to this headline">#</a></h2>
<p>The above methods are not ideal when many or all of the eigenvalues of a matrix are wanted; then a variety of more advanced methods have been developed, starting with the QR (factorization) method.</p>
<p>We will not address the details of that method in this course, but one way to think about it for a symmetric matrix is that:</p>
<ul class="simple">
<li><p>The eigenvectors are orthogonal.</p></li>
<li><p>Thus, if after computing <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\vec{v}_1\)</span>, one uses the power iteration starting with <span class="math notranslate nohighlight">\(\vec{x}^{\,0,2}\)</span> orthogonal to <span class="math notranslate nohighlight">\(\vec{v}_1\)</span>, then all the new iterates <span class="math notranslate nohighlight">\(\vec{x}^{\,k,2}\)</span> will stay orthogonal, and one will get the eigenvector corresponding to the largest remaining eigenvector: you get <span class="math notranslate nohighlight">\(\vec{v}_2\)</span> and <span class="math notranslate nohighlight">\(\lambda_2\)</span>.</p></li>
<li><p>Continuing likewise, one can get the eigenvalues in descending order of magnitude.</p></li>
<li><p>As a modification, one can do all these almost in parallel: at iteration <span class="math notranslate nohighlight">\(k\)</span>, have an approximation <span class="math notranslate nohighlight">\(\vec{x}^{\,k,i}\)</span> for each <span class="math notranslate nohighlight">\(\lambda_i\)</span> and at each stage, got by adjusting these new approximations so that <span class="math notranslate nohighlight">\(\vec{x}^{\,k,i}\)</span> is orthogonal to all the approximations <span class="math notranslate nohighlight">\(\vec{x}^{\,k,j}\)</span>, <span class="math notranslate nohighlight">\(j &lt; i\)</span>, for all the previous (larger) eigenvalues.
This uses a variant of the <a class="reference external" href="https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process">Gram-Schmidt</a> method for orthogonalizing a set of vectors.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">#</a></h2>
<section id="exercise-1">
<span id="id4"></span><h3>Exercise 1<a class="headerlink" href="#exercise-1" title="Permalink to this headline">#</a></h3>
<p>Implement the power method <a class="reference internal" href="#power-method">Algorithm 3.15</a> and test it on the real, symmetric matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split} A = \left[ \begin{array}{ccc} 3 &amp; 1 &amp; 1 \\ 1 &amp; 8 &amp; 1 \\ 1 &amp; 1 &amp; 4 \end{array} \right] \end{split}\]</div>
<p>This all real eigenvalues, all within <span class="math notranslate nohighlight">\(2\)</span> of the diagonal elements (this claim should be explained as part of the project write-up), so start with it.</p>
<p>As a debugging strategy, you could replace all those off-diagonal ones by a small value <span class="math notranslate nohighlight">\(\delta\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A_\delta = \left[ \begin{array}{ccc} 3 &amp; \delta &amp; \delta \\ \delta &amp; 8 &amp; \delta \\ \delta &amp; \delta &amp; 4 \end{array} \right]
\end{split}\]</div>
<p>Then the <a class="reference external" href="https://en.wikipedia.org/wiki/Gershgorin_circle_theorem">Gershgorin circle theorem</a>
ensures that each eigenvalue is within <span class="math notranslate nohighlight">\(2\delta\)</span> of an entry on the main diagonal.
Furthermore, if <span class="math notranslate nohighlight">\(\delta\)</span> is small enough that the circles of radius <span class="math notranslate nohighlight">\(2\delta\)</span> centered on the diagonal elements do not overlap,
then there is one eigenvalue in each circle.</p>
<p>You could even start with <span class="math notranslate nohighlight">\(\delta=0\)</span>, for which you know exactly the eigenvalues: they are the diagonal elements.</p>
</section>
<section id="exercise-2">
<span id="id5"></span><h3>Exercise 2<a class="headerlink" href="#exercise-2" title="Permalink to this headline">#</a></h3>
<p>Modify your code from <a class="reference external" href="#exercise-1">Exercise 1</a> to implement the stopping condition in
Equation <a class="reference internal" href="#equation-a-stopping-condition">(3.4)</a>.</p>
</section>
<section id="exercise-3">
<span id="id6"></span><h3>Exercise 3<a class="headerlink" href="#exercise-3" title="Permalink to this headline">#</a></h3>
<p>Implement the <a class="reference internal" href="#inverse-power-method">Inverse Power Method</a> (with a fixed iteration count, as in <a class="reference external" href="#exercise-1">Exercise 1</a> and then create a second version that imposes an accuracy target (as in <a class="reference external" href="#exercise-2">Exercise 2</a>).</p>
</section>
<section id="exercise-4">
<span id="id7"></span><h3>Exercise 4<a class="headerlink" href="#exercise-4" title="Permalink to this headline">#</a></h3>
<p>As above, implement the <a class="reference internal" href="#shifted-inverse-power-method"><span class="std std-ref">shifted inverse power method</span></a>,
probably starting with a fixed iteration count version.</p>
<p>For the test case above, some plausible initial choices for the shifts are each of the entries on the main diagonal, and as above, testing with <span class="math notranslate nohighlight">\(A_s\)</span>.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "julia-1.8"
        },
        kernelOptions: {
            kernelName: "julia-1.8",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'julia-1.8'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="linear-equations-7-tridiagonal-banded-and-SDD-matrices.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">3.8. </span>Faster Methods for Solving <span class="math notranslate nohighlight">\(Ax = b\)</span> for Tridiagonal and Banded matrices, and Strict Diagonal Dominance</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="newtons-method-for-systems-intro.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3.10. </span>Solving Nonlinear Systems of Equations by generalizations of Newton’s Method — a brief introduction</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Brenton LeMesurier (College of Charleston, South Carolina) with contributions from Stephen Roberts (Australian National University).<br/>
  
      &copy; Copyright 2021–2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>